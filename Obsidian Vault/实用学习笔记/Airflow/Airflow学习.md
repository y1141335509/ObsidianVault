## 1. ğŸ˜ˆæœ€ä¸å–œæ¬¢çš„é…ç¯å¢ƒ
å‚è€ƒæ•™ç¨‹ - [Airflowå®˜æ–¹](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)


ï¼ˆå¦‚æœä½ ä¹‹å‰å·²ç»åœ¨Dockerä¸­åˆ›å»ºäº†é¡¹ç›®ï¼Œé‚£ä½ å¯ä»¥è·³è¿‡å‰é¢çš„æ­¥éª¤ï¼‰
ğŸ³å¦‚æœæ˜¯åˆ›å»ºæ–°é¡¹ç›®ï¼Œé‚£ä¹ˆé¦–å…ˆæ‰“å¼€Docker


ğŸ’»æ‰“å¼€å‘½ä»¤è¡Œå¹¶`cd`åˆ°ä½ è¦åˆ›å»ºé¡¹ç›®çš„æ–‡ä»¶å¤¹è·¯å¾„ä¸‹


ğŸ’»åœ¨å‘½ä»¤è¡Œè¿è¡Œï¼š
```bash
docker run --rm "debian:bullseye-slim" bash -c 'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))'
```
è¯¥å‘½ä»¤é»˜è®¤ä¼šåˆ†é…4GBå†…å­˜ç©ºé—´ç»™Dockerï¼ˆç†æƒ³æƒ…å†µä¸‹æ˜¯åˆ†é…8GBï¼‰


ğŸ’»è·å–`docker-compose.yaml`æ–‡ä»¶ï¼š
```bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.8.1/docker-compose.yaml'
```
è¿è¡Œä¸Šé¢å‘½ä»¤ï¼Œä»è€Œè·å–`docker-compose.yaml`æ–‡ä»¶
ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€è¦æ›´æ”¹Airflowçš„ç‰ˆæœ¬å·ã€‚è¿™é‡Œçš„ç‰ˆæœ¬å·æ˜¯2.8.1ï¼‰


ğŸ’»åˆå§‹åŒ–ç¯å¢ƒï¼š
```bash
mkdir -p ./dags ./logs ./plugins ./config
echo -e "AIRFLOW_UID=$(id -u)" > .env
```
è¿™ä¼šåœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹åˆ›å»º4ä¸ªæ–°çš„æ–‡ä»¶å¤¹ 
* `dags` - è¯¥æ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾ä½ çš„DAGæ–‡ä»¶ã€‚é€šå¸¸æ¯ä¸ª.pyæ–‡ä»¶å°±æ˜¯ä¸€ä¸ªDAGæ–‡ä»¶ï¼Œè¿™æ˜¯Airflowé»˜è®¤çš„ç¼–ç¨‹è§„èŒƒ
* `logs` - è¯¥æ–‡ä»¶å¤¹ä¸­æœ‰task executionå’Œscheduler
* `plugins` -  ä½ å¯ä»¥æ·»åŠ custom log parseræˆ–è€…æ·»åŠ `airflow_local_settings.py`æ–‡ä»¶æ¥é…ç½®cluster policy
* `config` - ç”¨æ¥å­˜æ”¾ä½ çš„[custom plugins](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/plugins.html)


å¦‚æœä½ å‘ç°`AIRFLOW_UID`å°šæœªè¢«è®¾ç½®ï¼Œè™½ç„¶ä½ å¯ä»¥å¿½ç•¥å®ƒï¼Œä½†ä½ ä¹Ÿå¯ä»¥æŒ‡å®šIDçš„å€¼
```bash
AIRFLOW_UID=50000
```


æ¥ç€ï¼Œä½ éœ€è¦å®‰è£…Airflowæ‰€éœ€è¦çš„æ•°æ®åº“ã€‚é€šå¸¸é»˜è®¤æ•°æ®åº“æ˜¯sqlite3ï¼ˆéœ€è¦ç­‰15-30åˆ†é’Ÿï¼‰
```bash
docker compose up airflow-init
```


è¿è¡ŒAirflow
```bash
docker compuse up
```
å¯åŠ¨å®Œä¸è¦å…³é—­terminal


æ‰“å¼€vscodeï¼Œç„¶åæ‰“å¼€ä½ åˆ›å»ºçš„Airflowæ ¹æ–‡ä»¶å¤¹ï¼ˆä¹Ÿå°±æ˜¯ä½ ä¸Šé¢`cd`å»çš„æ–‡ä»¶å¤¹ï¼‰
ç„¶åè¿è¡Œ
```bash
docker compose run airflow-worker airflow info
```
æŸ¥çœ‹å„ç§åŒ…çš„ä¿¡æ¯


ï¼ˆå¦‚æœä½ ä¹‹å‰å·²ç»åœ¨Dockerä¸­åˆ›å»ºäº†é¡¹ç›®ï¼Œé‚£ä½ å¯ä»¥è·³è¿‡å‰é¢çš„æ­¥éª¤ï¼‰ç›´æ¥åœ¨vscodeä¸­è¿è¡Œï¼š
```bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.8.1/airflow.sh'
chmod +x airflow.sh
```
æˆ–è€…è¿è¡Œï¼š
```bash
./airflow.sh info
```


æ¥ä¸‹æ¥ï¼Œä½ å¯ä»¥åœ¨æµè§ˆå™¨ä¸Šè®¿é—®Airflowçš„UIï¼š
```bash
ENDPOINT_URL="http://localhost:8080/"
curl -X GET  \
    --user "airflow:airflow" \
    "${ENDPOINT_URL}/api/v1/pools"
```


ç„¶åä½ å¯ä»¥æ‰“å¼€æµè§ˆå™¨ï¼Œè¿›å…¥`http://localhost:8080`ã€‚å¦‚æœè¢«é—®åˆ°usernameå’Œpasswordï¼Œä¸”ä½ æ²¡æœ‰é¢„å…ˆè®¾ç½®ç”¨æˆ·åå’Œå¯†ç çš„è¯ï¼Œé»˜è®¤ä¸Šåˆ†åˆ«æ˜¯ airflowå’Œairflowï¼ˆå°å†™ï¼‰ã€‚å¦‚æ­¤ä½ å°±å¯ä»¥çœ‹åˆ°airflowçš„UIäº†ã€‚


å†ç»™ä¸€ä¸ªåˆ›å»ºDAGçš„æ¡ˆä¾‹ï¼š
åœ¨dagsæ–‡ä»¶å¤¹ä¸‹é¢åˆ›å»ºä¸€ä¸ª.pyæ–‡ä»¶ï¼Œç„¶åä¸‹é¢æ˜¯ä¸€ä¸ªä»£ç æ¡ˆä¾‹
![[Screenshot 2024-01-24 at 15.40.17.png]]
```python
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime, timedelta
import pytz

default_args = {
'owner': 'airflow',
'depends_on_past': False,
'start_date': datetime(2024, 1, 24, 16, 37, 0, tzinfo=pytz.timezone('America/Los_Angeles')),
'email_on_failure': False,
'email_on_retry': False,
'retries': 1,
'retry_delay': timedelta(minutes=5),
}

dag = DAG('my_first_dag',
default_args=default_args,
description='A simple tutorial DAG',
schedule_interval=timedelta(minutes=1),
catchup=False)

task1 = DummyOperator(
task_id='number1',
dag=dag,
)

task2 = DummyOperator(
task_id='number2',
dag=dag,
)

task3 = DummyOperator(
task_id='number3',
dag=dag,
)

task4 = DummyOperator(
task_id='number4',
dag=dag,
)

task5 = DummyOperator(
task_id='number5',
dag=dag,
) 

task1 >> [task2, task3]
task3 >> task4
task4 >> task5
```
ç‚¹è¿è¡Œåï¼Œç„¶åå»`http://localhost:8080/`ï¼Œæœç´¢`my_first_dag`å°±èƒ½çœ‹åˆ°åˆšåˆšåˆ›å»ºçš„DAGäº†ã€‚
ä¸Šé¢è¿™æ®µä»£ç æ˜¯æ¯åˆ†é’Ÿè§¦å‘çš„ï¼Œä½ å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´æœ€åˆè§¦å‘çš„æ—¶é—´





æœ€åï¼Œå¦‚æœä½ æƒ³è¦æ¸…ç†ä½ åˆšåˆšåˆ›å»ºçš„Airflowå’ŒDockeré¡¹ç›®ï¼Œå¯ä»¥åœ¨åœæ­¢Dockeråè¿è¡Œï¼š
```bash
docker compose down --volumes --rmi all
```

## 2. ä¸€äº›åŸºç¡€æ¦‚å¿µ

Operators Categories è¿è¡Œå™¨åˆ†ç±»
* `Sensors` - æ˜¯`Operators`çš„ä¸€ç§ï¼Œå®ƒä¼šåœ¨æ»¡è¶³æŸä¸ªæ¡ä»¶ä¹‹å‰ä¸€ç›´è¿è¡Œã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥ç”¨å®ƒç­‰å¾…ä¸Šæ¸¸æ•°æ®å‡†å¤‡å°±ç»ªåï¼Œæ‰§è¡Œåç»­çš„æ•°æ®å¤„ç†ç­‰ã€‚`å¸¸è§çš„Sensors`ï¼š
	* `HdfsSensor` - ç”¨æ¥ç­‰å¾…ä¸€ä¸ªæ–‡ä»¶æˆ–è€…æ–‡ä»¶å¤¹è¢«æ”¾åˆ°HDFSä¸­
	* `NameHivePartitionSensor` - æ£€æŸ¥æœ€è¿‘ä¸€æ¬¡çš„Hive tableçš„åˆ†åŒºæ˜¯å¦å¯ä»¥ç”¨ä½œä¸‹æ¸¸æ•°æ®å¤„ç†
* `Operators` - ç”¨æ¥è§¦å‘æŸç§â€œåŠ¨ä½œâ€ï¼ˆä¾‹å¦‚è¿è¡Œä¸€æ®µbashå‘½ä»¤ï¼Œæ‰§è¡Œä¸€æ®µpythonå‡½æ•°ï¼Œæˆ–è€…æ‰§è¡Œä¸€ä¸ªHive queryç­‰ï¼‰ã€‚å¸¸è§çš„Operatorsæœ‰ï¼š
	* `BashOperator` 
	* `PythonOperator` 
	* `HiveOperator` 
	* `BigQueryOperator` 
	éƒ½æ˜¯ç”¨æ¥æ‰§è¡Œç›¸åº”çš„å‘½ä»¤æˆ–ä»£ç ã€‚ä¸å¤šèµ˜è¿°ã€‚
* `Transfers` - ç”¨æ¥å°†æ•°æ®ä»ä¸€ä¸ªåœ°æ–¹è½¬ç§»åˆ°å¦ä¸€ä¸ªåœ°æ–¹ã€‚å¸¸è§çš„æœ‰ï¼š
	* `MySqlToHiveTransfer` 
	* `S3ToRedshiftTransfer`



### Spark

**Q: What are the key features of Apache Spark?**
A: "Apache Spark is known for its fast in-memory data processing capabilities. It offers high-level APIs in Java, Scala, Python, and R. Key features include Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for scalable, high-throughput, fault-tolerant stream processing of live data streams."

**Q: Can you explain RDDs in Spark?**
A: "RDDs, or Resilient Distributed Datasets, are the fundamental data structure of Spark. They are immutable, distributed collections of objects. RDDs can be created through deterministic operations on either data on disk or other RDDs. They are fault-tolerant as they track lineage information to rebuild lost data on node failure."

**Spark Core Concepts (RDDs, DataFrames, Transformations, Actions)**
- **RDDs**: Resilient Distributed Datasets (RDDs) are Sparkâ€™s fundamental data structure, offering fault-tolerant, parallel data processing. They allow users to perform in-memory computations on large clusters in a fault-tolerant manner.
- **DataFrames**: DataFrames in Spark are similar to those in pandas or R, optimized for big data processing and support various data formats. They are distributed collections of rows under named columns.
- **Transformations and Actions**: Transformations (like `map`, `filter`) create new RDDs/DataFrames and are lazily evaluated. Actions (like `count`, `collect`) trigger computation and return results.
- **Parallel Processing and Fault Tolerance**: Spark achieves parallel processing through distributed computing on clusters, and fault tolerance through lineage information of RDDs.
- **Lazy Evaluation**: Spark optimizes execution through lazy evaluation, meaning computations are only triggered when an action is called.


### Scala

**Q: What is Scala and why is it used with Spark?**
A: "Scala is a high-level, statically-typed programming language that combines functional and object-oriented programming paradigms. It's used with Spark for its concise syntax, functional programming capabilities, and because Sparkâ€™s native APIs are written in Scala. This provides more control and optimization opportunities in Spark data processing."

**Q: How does Scala improve Spark applications?**
A: "Scala can improve Spark applications' performance and readability. Its functional nature allows for writing concise and readable code. Scala's case classes and pattern matching facilitate easier manipulation of complex datasets. Moreover, being statically-typed, Scala can help catch errors at compile time, making Spark applications more robust."

**Basic Scala Syntax, Case Classes, Functional Programming**
- **Scala Syntax**: Scala combines object-oriented and functional programming. It supports immutable data structures, type inference, and concise syntax.
- **Case Classes**: Case classes in Scala are regular classes which are immutable by default and decomposable through pattern matching.
- **Functional Programming**: Scala supports higher-order functions, allows functions as values, and provides support for immutability and lazy evaluation.

**Integration of Airflow with Data Systems like Spark**
- **Airflow for Orchestration**: Airflow can orchestrate complex computational workflows, which can include Spark jobs. 
- **Scheduling Spark Jobs**: Airflow can schedule and monitor Spark jobs, manage dependencies, and handle retries and failures.

**Scala's Integration with Spark**
- **Native Support**: Spark is written in Scala, giving it native support for Scala, allowing more functional programming capabilities and concise code.
- **Dataset API**: Scalaâ€™s case classes and pattern matching enhance the Dataset API in Spark, improving the clarity and simplicity of complex data processing operations.

### Airflow

**Q: What is Apache Airflow and its use in data engineering?**
A: "Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. In data engineering, it's used for orchestrating complex computational workflows, ETL processes, and data pipelines. Airflowâ€™s scheduler executes tasks on an array of workers while following the specified dependencies."

**Q: How does Airflow manage task dependencies?**
A: "Airflow manages task dependencies using Directed Acyclic Graphs (DAGs). In a DAG, each node is a task, and edges define dependencies among these tasks. Airflow ensures that tasks are executed in the order respecting their dependencies and manages retries and failures."

**Scenarios for Using Airflow to Manage Data Pipelines**
- **ETL Processes**: Automating ETL tasks which involve extracting data from various sources, transforming it, and loading it into a data warehouse - Spark can process data, Airflow can manage the workflow.
- **Machine Learning Pipelines**: Orchestrating steps involved in machine learning pipelines, like data preprocessing, model training, and evaluation.


### System Design
**Q: What are Scalability and Fault Tolerance?** 
A:
* Scalability refers to the ability of a system to handle an increasing amount of work by adding resources. 
* Fault tolerance refers to the ability of a system to continue functioning even in the face of failures or errors.

**Scalability, Efficiency, and Reliability in System Design**
- **Scalability**: Ability to handle growing amounts of work or expanding in capacity.
- **Efficiency**: Optimal use of resources (like CPU, memory) to achieve performance goals.
- **Reliability**: System's ability to function under predefined conditions consistently over time.




### Hadoop

1. **What is Hadoop and its components?**
   Hadoop is an open-source framework for distributed storage and processing of large datasets using simple programming models. It primarily consists of Hadoop Distributed File System (HDFS) for storage, MapReduce for processing, and Yet Another Resource Negotiator (YARN) for cluster management.

2. **How does Hadoop differ from traditional databases?**
   Hadoop is designed for handling large volumes of unstructured or semi-structured data, unlike traditional databases which are optimized for structured data. Hadoop provides scalability and fault tolerance, whereas traditional databases focus more on transactional integrity and consistency.

3. **Explain Hadoop's architecture and its core components.**
   Hadoop's architecture includes HDFS for distributed storage, which splits data into blocks stored across multiple nodes. MapReduce processes data in parallel by dividing tasks into smaller sub-tasks. YARN manages resources in the cluster and schedules jobs.

4. **How does HDFS ensure data reliability and fault tolerance?**
   HDFS stores multiple copies of data blocks at different nodes (replication) to ensure data reliability and fault tolerance. If a node fails, data can be retrieved from another node.

5. **Describe YARN and its advantages.**
   YARN separates the resource management and job scheduling/monitoring functions. It enables better cluster utilization and supports multiple data processing engines, improving scalability and cluster efficiency.

6. **What are some common use cases for Hadoop?**
   Common use cases include big data analytics, data lakes, distributed data processing, and large-scale log or event data analysis.

7. **How do you handle data loss in HDFS?**
   Data loss in HDFS is handled through replication. If a node is lost, HDFS automatically replicates the data from other nodes to maintain the configured replication factor.

### MapReduce

1. **What is MapReduce and how does it work?**
   MapReduce is a programming model for processing large datasets in parallel across a Hadoop cluster. It works in two phases: the Map phase (where it processes and transforms data) and the Reduce phase (where it aggregates or summarizes data).

2. **Explain the difference between a 'map' and a 'reduce' task.**
   'Map' tasks process input data and produce key-value pairs as output. 'Reduce' tasks take these key-value pairs and aggregate them based on keys.

3. **How does MapReduce handle failure?**
   MapReduce handles failures by automatically rerunning failed tasks on different nodes. If a node fails, tasks are rescheduled on other available nodes.

4. **Describe a real-world scenario where you would use MapReduce.**
   A real-world scenario is log analysis, where MapReduce can process large volumes of log data to extract insights, such as user activity or error rates.

5. **What are the limitations of MapReduce?**
   MapReduce can be slow due to its batch processing nature and excessive disk I/O. It's not suitable for interactive data analysis or real-time processing.

6. **How does MapReduce fit into the Hadoop ecosystem?**
   MapReduce is a core component of the Hadoop ecosystem, providing a powerful framework for large-scale data processing and analysis on HDFS.

7. **Differences between Hadoop's MapReduce and Spark's computational model?**
   Spark performs in-memory processing, which is faster than MapReduce's disk-based processing. Spark also offers more flexibility with its DAG execution engine, compared to MapReduce's two-stage paradigm.

### Spark

1. **What is Apache Spark and how is it different from Hadoop?**
   Apache Spark is an open-source, distributed processing system used for big data workloads. It differs from Hadoop in offering in-memory processing, which is faster, and supports more than just MapReduce algorithms.

2. **Explain RDDs in Spark. How do they differ from DataFrames?**
   RDDs (Resilient Distributed Datasets) are a fundamental data structure of Spark that provides an immutable, distributed collection of objects. DataFrames are a higher-level abstraction built on RDDs, optimized for structured and semi-structured data, and provide more straightforward operations.

3. **What is lazy evaluation and how does Spark utilize it?**
   Lazy evaluation in Spark means that the execution will not start until an action is performed. This allows Spark to optimize the execution plan for efficiency.

4. **Describe Spark's fault tolerance mechanisms.**
   Spark achieves fault tolerance through lineage information. If a partition of an RDD is lost, Spark can rebuild it using the lineage graph.

5. **How does Spark achieve high performance and efficiency compared to MapReduce?**
   Spark achieves high performance through in-memory processing, reducing the need for disk I/O. Its DAG execution engine optimizes workflows, making it more efficient than MapReduce's linear approach.

6. **Can you explain Spark's execution model?**
   Spark's execution model involves creating a DAG of stages for each

6. **Can you explain Spark's execution model?**
   Spark's execution model involves creating a DAG (Directed Acyclic Graph) of task stages for each job. It divides the operators into stages of tasks that can be executed in parallel. This model allows for optimization and pipelining, which improves the overall efficiency of data processing.

7. **Discuss a use case where you would prefer Spark over Hadoop.**
   Spark is preferred for iterative algorithms in machine learning, interactive data mining, and real-time data processing due to its in-memory processing capability, which is much faster than Hadoop's disk-based processing.

### Scalability, Efficiency, and Reliability in System Design
- **Scalability**: The ability of a system to handle increased load by adding resources, either by scaling up (more powerful hardware) or scaling out (more machines).
- **Efficiency**: Efficient use of resources (like CPU, memory) to achieve high performance, including optimizing algorithms and reducing unnecessary processing.
- **Reliability**: The system's capability to operate correctly and consistently under defined conditions, including handling failures gracefully and ensuring data integrity.

These answers provide an overview of key concepts and practices in working with Hadoop, MapReduce, and Spark, offering insights into their functionalities, differences, and applications.



















